# just to play
# inspired by the code at https://github.com/pytorch/examples/blob/d91adc972cef0083231d22bcc75b7aaa30961863/fast_neural_style/neural_style/neural_style.py
# uses it NetTransformer model (tweaked) and the vgg style loss

import sys
import time
import os
import json
import pickle as pkl

import torch
import torchvision.transforms as Transforms
import albumentations as A
import cv2
import random

#from ..utils.config import getConfigFromDict, getDictFromConfig, BaseConfig
from utils.utils import rebase_path
from utils.visualize_image import save_visualization
from utils.image_transform import pil_loader, ToTensorV2
from attrib_dataset import AttribDataset

import importlib
import json

import numpy as np

import matplotlib.pyplot as plt

from nets.transformer_net import *

from nets.vgg import Vgg16

# make the image data loader
class DataLoaderTransformer:
    def __init__(self,
                 path_db, # path to database
                 albumentations_transformations=None,
                 selectedAttributes = None, # select according to certain attributes
                 size_compressed_input = None
                 ):
        # a wrapper class that invokes an Albumentations Compose, conditional on a 'size' argument
        self.albumentations_transformations = albumentations_transformations
        
        # path to dataset, separated into different class-directories
        self.path_db = path_db
        
        self.selectedAttributes = selectedAttributes
        
        # build key order
        self.attribKeysOrder = self.getDataset(size=10).getKeyOrders()
        print("AC;classes : ")
        print(self.attribKeysOrder)
        
        #
        if size_compressed_input is None:
            size_compressed_input = 64
            print("setting 'input' image size to %d" % size_compressed_input)
        self.size_compressed_input = size_compressed_input
    
    def getDBLoader(self, size, bs, num_workers = None, shuffle=None):
        r"""
        Load the training dataset for the given scale.
        Args:
            - scale (int): scale at which we are working
        Returns:
            A dataset with properly resized inputs.
        """
        if shuffle is None:
            shuffle =True
        
        if num_workers is None:
            num_workers = 1
        
        dataset = self.getDataset(size)
        
        return torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=shuffle, num_workers=num_workers) 
    
    def getDataset(self, size=None):
        
        if size is None:
            size = 512
        
        print("size", size)
        if self.albumentations_transformations is None:
            raise NotImplementedError("need to provide a class of Albumentations Compose called argument 'albumentations_transformations'")
        
        transform = self.albumentations_transformations(size)
        
        return AttribDataset(self.path_db,
                             transform=transform,
                             attribDictPath=None, #self.pathAttribDict,
                             specificAttrib=self.selectedAttributes,
                             mimicImageFolder=True)#self.imagefolderDataset)
    
    def downscale_fullsize_target_to_small_input(self, target_images, size_compressed_input=None):
        """ take the target image (generally 512x512) and downscale it to 64x64"""
        if size_compressed_input is None:
            size_compressed_input = self.size_compressed_input
            if size_compressed_input is None:
                raise AttributeError("'size_compressed_input' is None; must set to ~64")
        # change to Numpy
        #target_image = target_image.detach().numpy()
        return Transforms.Resize(size_compressed_input, interpolation=2)(target_images)

class AlbumentationsTransformations(object):
    """ Wrapper class used to re-initialize a Albumentations Compose, conditional on changing parameters of 'size' """
    def __init__(self, size):
        if isinstance(size,int):
            size = (size,size)
        self.A_Compose = A.Compose([A.ChannelShuffle(always_apply=False, p=0.1),
                                    A.RandomGridShuffle(always_apply=False, p=0.05, grid=(2, 2)),
                       A.transforms.GridDistortion(num_steps=5, distort_limit=0.4, p=0.6),
                       A.HorizontalFlip(p=0.5),
                       A.RandomResizedCrop(always_apply=True, p=1.0, height=size[0], width=size[0],
                                         scale=(0.75, 1),
                                         ratio=(0.97, 1.03), interpolation=3),#ONLY 3 SEEMS TO WORK
                       A.transforms.ColorJitter(brightness=0.05, contrast=0.07, saturation=0.04, hue=0.08, always_apply=False, p=0.7),#HSV random
                       A.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), 
                       ToTensorV2()
                       ])
    
    def __call__(self, PIL_image):
        transformed_image_dict = self.A_Compose(image = np.array(PIL_image))
        return transformed_image_dict['image']
    
    def __repr__(self):
        format_string = self.__class__.__name__ + '('
        for t in self.transforms:
            format_string += '\n'
            format_string += '    {0}'.format(t)
        format_string += '\n)'
        return format_string

def gram_matrix(y):
    """ gram matrix for style loss"""
    (b, ch, h, w) = y.size()
    features = y.view(b, ch, w * h)
    features_t = features.transpose(1, 2)
    gram = features.bmm(features_t) / (ch * h * w)
    return gram

def style_lossf(gram_style_fullsize, gram_style_reconstruction):
    style_loss = 0.
    # loop through vgg features
    for gm_o, gm_r in zip(gram_style_fullsize, gram_style_reconstruction):
        style_loss += mse_loss(gm_o, gm_r)
    
    return style_loss 

mse_loss = torch.nn.MSELoss()

path_db = '/tmp/data/'
size_compressed_input = (64,64)

# initialize the db loader class
db_loader = DataLoaderTransformer(
    path_db = path_db, #'/tmp/data/', 
    albumentations_transformations=AlbumentationsTransformations,
    selectedAttributes=None,
    size_compressed_input = size_compressed_input)

# get the loader... (loads images, etc)
db_iter = db_loader.getDBLoader(size=512, bs=3, num_workers = 1, shuffle=True)

for bitem, (by_full, blabels) in enumerate(db_iter):
    #lkjlk
    
    # compress the image to 64x64  (serves as the input image
    bx_comp = db_loader.downscale_fullsize_target_to_small_input(target_images = by_full)

# make the target image
#target_image = bx_full.detach().numpy()
#foo = Transforms.Resize(size_compressed_input, interpolation=2)(bx_full)
#plt.imshow(foo[0][0]);plt.show()

# make a transformer net
net = TransformerNet()

# keeps everything the same size
yr = net.conv1.reflection_pad(bx_comp)
yr = net.conv1.conv2d(yr)

y = net.relu(net.in1(net.conv1(bx_comp))) # [bs,64,]->[bs,32,64,]
y = net.relu(net.in2(net.conv2(y))) # [bs,32,64,]->[bs,64,32,32]
y = net.relu(net.in3(net.conv3(y))) # [bs,64,32,32]-> [bs, 128, 16, 16]
y = net.res1(y) # [3, 128, 16, 16]
y = net.res2(y) # [3, 128, 16, 16]
y = net.res3(y) # [3, 128, 16, 16]
y = net.res4(y) # [3, 128, 16, 16]
y = net.res5(y) # [3, 128, 16, 16]
y = net.relu(net.in4(net.deconv1(y))) # [3, 64, 32, 32]
y = net.relu(net.in5(net.deconv2(y))) # [3, 32, 64, 64]
y = net.deconv3(y) # [3, 3, 64, 64]

# ideas: net.conv1 -> net.conv2 (skip net.conv3)
y = net.relu(net.in1(net.conv1(bx_comp))) # [bs,64,] -> [bs,32,64,]
y = net.relu(net.in2(net.conv2(y))) # [bs,32,64,] -> [bs,64,32,32]
y = net.relu(net.in3(net.conv3(y))) # [bs,64,32,32] -> [bs, 128, 16, 16]
y = net.res1(y) # [3, 128, 16, 16]
y = net.res2(y) # [3, 128, 16, 16]
y = net.res3(y) # [3, 128, 16, 16]
y = net.res4(y) # [3, 128, 16, 16]
y = net.res5(y) # [3, 128, 16, 16]
y = net.relu(net.in4(net.deconv1(y))) # [3, 64, 32, 32]
y_dim_orig = net.relu(net.in5(net.deconv2(y))) # [3, 32, 64, 64]
# concatenate [bx_comp, y_dim_at_orig]
y_dim_orig_cat = torch.cat([bx_comp, y_dim_orig], axis = 1)  # [3, 35, 64, 64]
# upscale new
y_dim_2x = net.relu(net.in6(net.deconv3(y_dim_orig_cat))) # [3, 32, 128, 128]
# upscale new 2
y_dim_4x = net.relu(net.in7(net.deconv4(y_dim_2x))) # [3, 32, 256, 256]
# upscale new 3
y_dim_8x = net.relu(net.in8(net.deconv4(y_dim_4x))) # [3, 32, 512, 512]
# final out

out = net.deconv3(y_dim_8x) # [3, 3, 64, 64]

params_dict ={'input_inchannel_size':3,
              'conv1_kernel_size1':9,
              'conv1_extra_padding':0,
              'resblock_dim':64,
              'deconv1_inchannel_size':64,
              'deconv1_outchannel_size': 64,
              'deconv2_inchannel_size':64,
              'deconv2_outchannel_size': 64,
              'deconv3_inchannel_size':64,
              'deconv3_outchannel_size': 32,
              'downsample_factor':64/512,
              'downsample_mode':'nearest'}
              #'deconv4_inchannel_size':64,
              #'deconv4_outchannel_size': 32}
params = HyperParams(params_dict)

device = torch.device("cuda" if False else "cpu")

vgg = Vgg16(requires_grad=False).to(device)

#net2 = TransformerNet2(params).to(device)
net2 = TransformerNet3(params).to(device)

out,out_64 = net2(bx_comp)

#plt.imshow(out.detach().numpy()[0][0]);plt.show()

X_upsampled_x4 = torch.nn.functional.interpolate(bx_comp, mode='nearest', scale_factor=4)
# convolution

y = net2.relu(net2.in1(net2.conv1(bx_comp)))
y = net2.relu(net2.in2(net2.conv2(y)))
y = net2.res1(y)
y = net2.res2(y)
y = net2.res3(y)
y_back_to_orig_dim = net2.relu(net2.in4(net2.deconv1(y))) # in[32,32] out [64,64]
y_cat = torch.cat([X, y_back_to_orig_dim],axis=1)
y_dim_x4 = net2.relu(net2.in5(net2.deconv2(y_cat))) # in [64,64] out [256,256]
y_cat = torch.cat([X_upsampled_x4, y_dim_x4], axis=1)
y_dim_x8 = net2.relu(net2.in6(net2.deconv3(y_cat))) # in [256,256] out[512,512]
out = net2.deconv_out(y_dim_x8)

# specific the losses
mse_loss = torch.nn.MSELoss()

# try the vgg loss
#vgg = Vgg16(requires_grad=False)

# style of the input
features_style_fullsize = vgg(by_full)
gram_style_fullsize = [gram_matrix(y) for y in features_style_fullsize]
# style of the reconstruction
features_style_reconstruction = vgg(out)
gram_style_reconstruction = [gram_matrix(y) for y in features_style_reconstruction]

style_loss = style_lossf(gram_style_fullsize, gram_style_reconstruction)

save_dir = "models_trained/v2/";
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)

target_size = 512
n_epochs = 10
bs = 3
lr = 0.001
optimizer = torch.optim.Adam(net2.parameters(),lr = lr)
wts = {'style':10, 'pixel':0.002, 'content':0.001}
epoch = -1

while epoch < n_epochs:
    epoch+=1
    
    db_iter = db_loader.getDBLoader(size=target_size, bs=bs, num_workers = 1, shuffle=True)
    # loop through the images
    for bitem, (by_full, blabels) in enumerate(db_iter):
        
        # compress the image to 64x64  (serves as the input image
        bx_comp = db_loader.downscale_fullsize_target_to_small_input(target_images = by_full)
        
        optimizer.zero_grad()
        
        # reconstruct image
        out, out_downsampled = net2(bx_comp)
        
        # VGG features
        features_style_fullsize = vgg(by_full)
        features_style_reconstruction = vgg(out)
        
        # gram matrices for style loss
        gram_style_fullsize = [gram_matrix(y) for y in features_style_fullsize]
        # style of the reconstruction
        gram_style_reconstruction = [gram_matrix(y) for y in features_style_reconstruction]
        # style loss
        bstyle_loss = style_lossf(gram_style_fullsize, gram_style_reconstruction)
        
        # pixel loss
        pixel_loss = mse_loss(by_full, out)
        
        # content loss (on downsampled reconstruction)
        content_loss = mse_loss(bx_comp, out_downsampled)
        
        #bstyle_loss.backward() # takes a long time
        rwts_pixel = wts['pixel']*(random.random()<0.95);
        rwts_content = wts['content']*(random.random()<0.95)
        
        (rwts_pixel*pixel_loss + wts['style']*bstyle_loss + rwts_content*content_loss).backward()
        optimizer.step()
        if bitem % 10 ==0:
            print("E%d; STEP%d; style:%0.6f; content:%0.3f; pix:%0.3f" % (epoch, bitem, bstyle_loss.item(), content_loss.item(), pixel_loss.item()))
            time.sleep(30)
        #if epoch %3 ==0 and epoch>10:
    
    if True:
        if True:
            print("saving model and making a dummy image")
            path_to_model = "%sstyletransfer_v1_e%d.model" % (save_dir, epoch)
            torch.save({
                'epoch': epoch,
                'model_state_dict': net2.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'style_loss':bstyle_loss.item(),
                'content_loss':content_loss.item(),
                'pixel_loss':pixel_loss.item(),
                'lr':lr,
                'weights':wts,
                'bs':bs,
                'params_dict':params_dict},path_to_model)
            # plot images
            save_visualization(by_full, out, vis_path=save_dir, suffix = "E%d" % epoch)
            time.sleep(120)
            print("resuming in 30 seconds...")
            time.sleep(30)
            

# try a few visualizations
import scipy
import scipy.misc
from PIL import Image


#def make_numpy_grid(arrays_list, gridMaxWidth=2048,
#                    imgMinSize=128,
#                    interpolation='nearest'):
arrays_list = by_full.detach().numpy()
# NCWH format
def save_visualization(by_full, out, vis_path, suffix):
    vgg_std = (0.229, 0.224, 0.225)
    vgg_mu = (0.485, 0.456, 0.406)
    post_proc = []
    for arrays_list in (by_full,out):
        arrays_list = arrays_list.detach().numpy()
        N, C, W, H = arrays_list.shape
        # de-normalize the transofrmed data: (y*s + m)*255
        # make a standard deviation array
        std_ = np.array(vgg_std).repeat((N*W*H)).reshape((C,N,W,H)).transpose(1,0,2,3)
        # make a standard deviation array
        mu_ = np.array(vgg_mu).repeat((N*W*H)).reshape((C,N,W,H)).transpose(1,0,2,3)
        # backtransform from vgg normalization
        arrays_list= ((arrays_list*std_+mu_)*255).clip(0,255).astype(np.uint8)
        # append to post-processed
        post_proc.append(arrays_list)
    
    # concatenate
    img_cat = np.concatenate(post_proc, axis = 2)
    for indexImage in range(N):
        # convert to image
        img = np.array(img_cat[indexImage])
        img = Image.fromarray(np.transpose(img, (1,2,0)))
        # save
        path_to_saved_image = os.path.join(vis_path,"eval_%s_%d.jpg" % (suffix, indexImage))
        img.save(path_to_saved_image, "JPEG", quality=80, optimize=True, progressive=True)

######################
# this version uses  the vgg content loss (instead of the compression-content loss)
mse_loss = torch.nn.MSELoss()

path_db = '/tmp/data/'
size_compressed_input = (64,64)

# initialize the db loader class
db_loader = DataLoaderTransformer(
    path_db = path_db, #'/tmp/data/', 
    albumentations_transformations=AlbumentationsTransformations,
    selectedAttributes=None,
    size_compressed_input = size_compressed_input)


params_dict ={'input_inchannel_size':3,
              'conv1_kernel_size1':9,
              'conv1_extra_padding':0,
              'resblock_dim':64,
              'deconv1_inchannel_size':64,
              'deconv1_outchannel_size': 64,
              'deconv2_inchannel_size':64,
              'deconv2_outchannel_size': 64,
              'deconv3_inchannel_size':64,
              'deconv3_outchannel_size': 32,
              'downsample_factor':64/512,
              'downsample_mode':'nearest'}
              #'deconv4_inchannel_size':64,
              #'deconv4_outchannel_size': 32}

params = HyperParams(params_dict)

device = torch.device("cuda" if False else "cpu")

vgg = Vgg16(requires_grad=False).to(device)

net2 = TransformerNet4(params).to(device)

save_dir = "models_trained/v2/";
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)

target_size = 512
n_epochs = 10
bs = 3
lr = 0.001
wts = {'style':1e10, 'pixel':512.0**-0.5, 'content':1e5}
epoch = -1
do_reload = True

while epoch < n_epochs:
    epoch+=1
    
    db_iter = db_loader.getDBLoader(size=target_size, bs=bs, num_workers = 1, shuffle=True)
    # loop through the images
    for bitem, (by_full, blabels) in enumerate(db_iter):
        
        # compress the image to 64x64  (serves as the input image
        bx_comp = db_loader.downscale_fullsize_target_to_small_input(target_images = by_full)
        
        optimizer.zero_grad()
        
        # reconstruct image
        out, out_downsampled = net2(bx_comp)
        
        # VGG features
        features_style_fullsize = vgg(by_full)
        features_style_reconstruction = vgg(out)
        
        # gram matrices for style loss
        gram_style_fullsize = [gram_matrix(y) for y in features_style_fullsize]
        # style of the reconstruction
        gram_style_reconstruction = [gram_matrix(y) for y in features_style_reconstruction]
        # style loss
        bstyle_loss = style_lossf(gram_style_fullsize, gram_style_reconstruction)
        
        # pixel loss
        pixel_loss = mse_loss(by_full, out)
        
        # content loss (vgg deeper features
        #content_loss = mse_loss(bx_comp, out_downsampled)
        content_loss = mse_loss(features_style_fullsize.relu2_2, features_style_reconstruction.relu2_2)
        
        #bstyle_loss.backward() # takes a long time
        rwts_pixel = wts['pixel']*(random.random()<0.95);
        #rwts_content = wts['content']*(random.random()<0.95)
        
        (rwts_pixel*pixel_loss + wts['style']*bstyle_loss + wts['content']*content_loss).backward()
        optimizer.step()
        if bitem % 10 ==0:
            print("E%d; STEP%d; style:%0.6f; content:%0.3f; pix:%0.3f" % (epoch, bitem, bstyle_loss.item(), content_loss.item(), pixel_loss.item()))
            time.sleep(30)
        #if epoch %3 ==0 and epoch>10:
    
    if True:
        print("saving model and making a dummy image")
        path_to_model = "%sstyletransfer_v1_e%d.model" % (save_dir, epoch)
        torch.save({
                'epoch': epoch,
                'model_state_dict': net2.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'style_loss':bstyle_loss.item(),
                'content_loss':content_loss.item(),
                'pixel_loss':pixel_loss.item(),
                'lr':lr,
                'weights':wts,
                'bs':bs,
                'params_dict':params_dict},path_to_model)
        # plot images
        save_visualization(by_full, out, vis_path=save_dir, suffix = "E%d" % epoch)
        time.sleep(120)
        print("resuming in 30 seconds...")
        time.sleep(30)


##########################
# run the train.py
import sys
import time
import os
import json
import pickle as pkl
import torch
import torchvision.transforms as Transforms
import albumentations as A
import cv2
import random

os.chdir("/media/AURA/Documents/ScriptsPrograms/ml_art/SuperResAutoEncoder/src")
sys.path.append("/media/AURA/Documents/ScriptsPrograms/ml_art/SuperResAutoEncoder/src")

from utils.utils import rebase_path
from utils.visualize_image import save_visualization
from utils.image_transform import pil_loader, ToTensorV2
from attrib_dataset import AttribDataset
import importlib
import json
import numpy as np
import matplotlib.pyplot as plt
from nets.transformer_net import *
from nets.vgg import Vgg16

from train import *

path_db = '/tmp/data/'
params_dict= {'input_inchannel_size':3,
              'conv1_kernel_size1':9,
              'conv1_extra_padding':0,
              'resblock_dim':64,
              'deconv1_inchannel_size':64,
              'deconv1_outchannel_size': 64,
              'deconv2_inchannel_size':64,
              'deconv2_outchannel_size': 64,
              'deconv3_inchannel_size':64,
              'deconv3_outchannel_size': 32,
              'downsample_factor':64/512,
              'downsample_mode':'nearest'}
save_dir = "models_trained/v2/"
do_cuda = False
do_reload=True
n_epochs = 10
bs = 3
lr = 0.001
wts = {'style':1e10, 'pixel':512.0**-0.5, 'content':1e5}
path_to_reload_model = "%sstyletransfer_v1_e%d.model" % (save_dir, 2)
os.path.isfile(path_to_reload_model)

train(params_dict = params_dict,
      path_db = path_db,
      save_dir=save_dir,
      do_cuda = do_cuda,
      do_reload=do_reload,
      n_epochs =n_epochs,
      bs = bs,
      lr = lr,
      wts = wts,
      path_to_reload_model = path_to_reload_model)
